{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-11T08:43:50.669881Z",
     "start_time": "2024-09-11T08:43:48.926421Z"
    }
   },
   "source": [
    "import json\n",
    "from os import mkdir\n",
    "import csv\n",
    "from utils import get_files, exists, join\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import krippendorff"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Gold Annotations",
   "id": "5c8a2dba5b1a7352"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:43:50.686325Z",
     "start_time": "2024-09-11T08:43:50.675352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_json(filePath):\n",
    "    with open(filePath, 'r') as fh:\n",
    "        jsonData = json.load(fh)\n",
    "    return jsonData\n",
    "    \n",
    " \n",
    "def write_json(outputFolder, fileName, jsonData):\n",
    "    with open(join(outputFolder, fileName), 'w') as fh:\n",
    "        json.dump(jsonData, fh, sort_keys=True, indent=4)   \n",
    "\n",
    "def make_dir(path):\n",
    "    if not exists(path):\n",
    "        mkdir(path)\n",
    "\n",
    "corpus_dir = \"corpus-json\"\n",
    "output_dir = \"gold-annotations\"\n",
    "make_dir(output_dir)\n",
    "make_dir(join(output_dir, \"adjudicated_gold\"))\n",
    "make_dir(join(output_dir, \"too_add_adjudicated_gold\"))\n",
    "files = get_files(corpus_dir)"
   ],
   "id": "4cc08b37d3cb4117",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:43:56.535742Z",
     "start_time": "2024-09-11T08:43:56.513214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def categorize_annotations(annotations):\n",
    "    # Create dictionaries to store counts of each unique annotation\n",
    "    counts = {}\n",
    "\n",
    "    # Separate entities and relationships\n",
    "    for annotation in annotations:\n",
    "        if 'startOffset' in annotation:  # It's an entity\n",
    "            key = (annotation['startOffset'], annotation['endOffset'], tuple(annotation['tags']), annotation['textProvided'])\n",
    "        elif 'from' in annotation:  # It's a relationship\n",
    "            key = (annotation['from_text'], annotation['to_text'], tuple(annotation['tags']))\n",
    "\n",
    "        if key not in counts:\n",
    "            counts[key] = []\n",
    "        counts[key].append(annotation)\n",
    "\n",
    "    matched_annotations = []\n",
    "    mismatched_annotations = []\n",
    "\n",
    "    # Separate matched and mismatched annotations\n",
    "    for key, value in counts.items():\n",
    "        if len(value) >=2:  # All 3 annotators agree\n",
    "            matched_annotations.extend(value)\n",
    "        else:\n",
    "            mismatched_annotations.extend(value)\n",
    "\n",
    "    return process_annotations(matched_annotations), mismatched_annotations\n",
    "\n",
    "def process_annotations(annotations):\n",
    "    unique_annotations = []\n",
    "    seen_annotations = set()\n",
    "\n",
    "    for annotation in annotations:\n",
    "        # Create a reduced annotation without the 'annotator' key\n",
    "        if 'startOffset' in annotation:  # It's an entity\n",
    "            reduced_annotation = {\n",
    "                'id': annotation['id'],\n",
    "                'startOffset': annotation['startOffset'],\n",
    "                'endOffset': annotation['endOffset'],\n",
    "                'tags': annotation['tags'],\n",
    "                'textProvided': annotation['textProvided']\n",
    "            }\n",
    "            annotation_tuple = (reduced_annotation['startOffset'], reduced_annotation['endOffset'],\n",
    "                                tuple(reduced_annotation['tags']), reduced_annotation['textProvided'])\n",
    "        elif 'from' in annotation:  # It's a relationship\n",
    "            reduced_annotation = {\n",
    "                'from': annotation['from'],\n",
    "                'from_text': annotation['from_text'],\n",
    "                'to': annotation['to'],\n",
    "                'to_text': annotation['to_text'],\n",
    "                'tags': annotation['tags']\n",
    "            }\n",
    "            annotation_tuple = (reduced_annotation['from_text'], reduced_annotation['to_text'],\n",
    "                                tuple(reduced_annotation['tags']))\n",
    "\n",
    "        # Check if this annotation is unique\n",
    "        if annotation_tuple not in seen_annotations:\n",
    "            seen_annotations.add(annotation_tuple)\n",
    "            unique_annotations.append(reduced_annotation)\n",
    "\n",
    "    return unique_annotations\n",
    "\n",
    "def write_mismatched_to_tsv(file_path, mismatched_annotations):\n",
    "    # Define TSV headers\n",
    "    headers = [\"id\",'Annotator', 'Type', 'StartOffset', 'EndOffset', 'From', 'To', 'Tags', 'TextProvided']\n",
    "\n",
    "    # Prepare data for TSV\n",
    "    rows = []\n",
    "    for annotation in mismatched_annotations:\n",
    "        row = []\n",
    "        if 'startOffset' in annotation:  # Entity annotation\n",
    "            row = [\n",
    "                annotation['id'],\n",
    "                annotation['annotator'],\n",
    "                'Entity',\n",
    "                annotation['startOffset'],\n",
    "                annotation['endOffset'],\n",
    "                '',  # No 'from' for entities\n",
    "                '',  # No 'to' for entities\n",
    "                ','.join(annotation['tags']),\n",
    "                annotation['textProvided']\n",
    "            ]\n",
    "        elif 'from' in annotation:  # Relationship annotation\n",
    "            row = [\n",
    "                \"\",\n",
    "                annotation['annotator'],\n",
    "                'Relationship',\n",
    "                annotation['from'],\n",
    "                annotation['to'], \n",
    "                annotation['from_text'],\n",
    "                annotation['to_text'],\n",
    "                ','.join(annotation['tags']),\n",
    "                ''  # No 'textProvided' for relationships\n",
    "            ]\n",
    "        if row != []:\n",
    "            rows.append(row)\n",
    "\n",
    "    # Write to TSV\n",
    "    with open(file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter='\\t')\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(rows)"
   ],
   "id": "109fb8e39a283f1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T14:49:42.950867Z",
     "start_time": "2024-09-04T14:49:42.453940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file in files:\n",
    "    try:\n",
    "        jsonData = read_json(join(corpus_dir, file))\n",
    "        adjudicated_gold = categorize_annotations(jsonData)\n",
    "        write_json(join(output_dir, \"adjudicated_gold\"), f\"gold_{file.replace('.txt', '')}\", adjudicated_gold[0])\n",
    "        #write_json(join(output_dir, \"too_add_adjudicated_gold\"), f\"to_add_gold_{file}\", adjudicated_gold[1])\n",
    "        write_mismatched_to_tsv(join(output_dir, \"too_add_adjudicated_gold\", f\"to_add_{file}.tsv\".replace(\".txt.json\", \"\")), adjudicated_gold[1])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {file} error: {e}\")"
   ],
   "id": "50026a4c2ba03df1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed: .DS_Store error: 'utf-8' codec can't decode byte 0x80 in position 3131: invalid start byte\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8e6675bb7b54d3e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T13:00:08.190963Z",
     "start_time": "2024-09-02T13:00:08.187085Z"
    }
   },
   "cell_type": "code",
   "source": "filePath = \"iaa.json\"",
   "id": "a0a129ab21b5a55c",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:44:06.009968Z",
     "start_time": "2024-09-11T08:44:05.894273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filePath = \"iaa.json\"\n",
    "\n",
    "def sort_annotation_by_id(data):\n",
    "    return sorted(data, key=lambda x: x['id'])\n",
    "\n",
    "def read_json(filePath):\n",
    "    with open(filePath, 'r') as fh:\n",
    "        jsonData = json.load(fh)\n",
    "    annotater1 = sort_annotation_by_id([i for i in jsonData if i['annotator'] == 1])\n",
    "    annotater2 = sort_annotation_by_id([i for i in jsonData if i['annotator'] == 2])\n",
    "    annotater3 = sort_annotation_by_id([i for i in jsonData if i['annotator'] == 3])\n",
    "    return [annotater1, annotater2, annotater3]\n",
    "\n",
    "def get_labels_for_all_annotator(annotations):\n",
    "    df = pd.DataFrame()\n",
    "    temp = pd.DataFrame()\n",
    "    for annotator in annotations:\n",
    "        for i in annotator:\n",
    "            temp['id'] = [i['id']]\n",
    "            temp['annotator'] = [i['annotator']]\n",
    "            temp['label_text'] = [get_labels_and_text(i)]\n",
    "            df = pd.concat([df, temp])\n",
    "    t = list()\n",
    "    for i,row in df.iterrows():\n",
    "        t.append([r[0] for r in row['label_text']])\n",
    "    flattened_list = [item for sublist in t for item in sublist]\n",
    "    entites = set(flattened_list)\n",
    "    return [df.reset_index(drop=True), entites]\n",
    "\n",
    "def get_labels_and_text(annotations):\n",
    "    return [[ann['hypertextlabels'][0], ann['text'] ] for ann in annotations['ner']]\n",
    "\n",
    "def get_entities_labels_grouped(df, entities):\n",
    "    final_df = pd.DataFrame()\n",
    "    for i in sorted(df.id.unique()):\n",
    "        temp = df[df['id'] == i].reset_index(drop=True)\n",
    "        for annotator in [1,2,3]:\n",
    "            temp_annotator = pd.DataFrame()\n",
    "            annotator_row = temp[temp['annotator'] == annotator].reset_index(drop=True)\n",
    "            temp_annotator['annotator'] = [annotator]\n",
    "            for entity in entities:\n",
    "                temp_annotator['entity'] = entity\n",
    "                entity_labeled_text = list()\n",
    "                for lt in annotator_row['label_text'][0]:\n",
    "                    if lt[0] == entity:\n",
    "                        entity_labeled_text.append(lt[1])\n",
    "                temp_annotator['entity_label_text'] = [entity_labeled_text]\n",
    "                final_df = pd.concat([final_df, temp_annotator]).reset_index(drop=True)\n",
    "    return final_df.groupby(['annotator', 'entity'])['entity_label_text'].agg(lambda x: flatten_list_of_lists(x)).reset_index()\n",
    "\n",
    "def flatten_list_of_lists(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def prepare_annotations_binary(df, entity):\n",
    "    entity_df = df[df['entity'] == entity]\n",
    "    return entity_df['binary_vector'].tolist()\n",
    "def create_binary_vector(labels, all_labels):\n",
    "    return [1 if label in labels else 0 for label in all_labels]\n",
    "\n",
    "def binary_each_entity(df):# Create a binary vector for each annotator and entity\n",
    "    unique_labels = df.groupby('entity')['entity_label_text'].apply(lambda x: set([label for sublist in x for label in sublist]))\n",
    "    binary_vectors = []\n",
    "    for _, row in df.iterrows():\n",
    "        all_labels = unique_labels[row['entity']]\n",
    "        binary_vector = create_binary_vector(row['entity_label_text'], all_labels)\n",
    "        binary_vectors.append(binary_vector)\n",
    "    df['binary_vector'] = binary_vectors\n",
    "    return df\n",
    "\n",
    "def kappa_score(df):\n",
    "    df = binary_each_entity(df)\n",
    "    entities = df['entity'].unique()\n",
    "    alpha_scores = {}\n",
    "    \n",
    "    for entity in entities:\n",
    "        if entity == 'model_id' or entity == 'cancer_stage' or entity == \"host_strain\":\n",
    "            continue\n",
    "        print(entity)\n",
    "        annotations = prepare_annotations_binary(df, entity)\n",
    "        alpha = krippendorff.alpha(reliability_data=annotations, level_of_measurement='nominal')\n",
    "        alpha_scores[entity] = alpha\n",
    "    return alpha_scores"
   ],
   "id": "4a9de7947f43aa40",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T13:00:23.801570Z",
     "start_time": "2024-09-02T13:00:23.694038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sorted_annotations = read_json(filePath)\n",
    "split_set = pd.read_csv(\"Corpus-split - positive.csv\")\n",
    "df, entities = get_labels_for_all_annotator(sorted_annotations, split_set)\n",
    "df = get_entities_labels_grouped(df, entities)\n",
    "alpha_scores = kappa_score(df)"
   ],
   "id": "452d35a787721ac2",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_labels_for_all_annotator() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[60], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m sorted_annotations \u001B[38;5;241m=\u001B[39m read_json(filePath)\n\u001B[1;32m      2\u001B[0m split_set \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCorpus-split - positive.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m df, entities \u001B[38;5;241m=\u001B[39m \u001B[43mget_labels_for_all_annotator\u001B[49m\u001B[43m(\u001B[49m\u001B[43msorted_annotations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit_set\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m df \u001B[38;5;241m=\u001B[39m get_entities_labels_grouped(df, entities)\n\u001B[1;32m      5\u001B[0m alpha_scores \u001B[38;5;241m=\u001B[39m kappa_score(df)\n",
      "\u001B[0;31mTypeError\u001B[0m: get_labels_for_all_annotator() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T08:10:43.047943Z",
     "start_time": "2024-08-27T08:10:43.041672Z"
    }
   },
   "cell_type": "code",
   "source": "alpha_scores",
   "id": "7ef8f95875b3a483",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age_category': -0.07407407407407396,\n",
       " 'biomarker': 0.20531222659667536,\n",
       " 'cancer_grade': 0.14814814814814825,\n",
       " 'cancer_stage': 0.375,\n",
       " 'clinical_trial': 0.11982082866741317,\n",
       " 'diagnosis': 0.23908056358381502,\n",
       " 'genetic_effect': 0.1853098890135928,\n",
       " 'host_strain': 0.2222222222222222,\n",
       " 'model_type': 0.06412797934627268,\n",
       " 'molecular_char': 0.09920987654320979,\n",
       " 'response_to_treatment': 0.005229945736243802,\n",
       " 'sample_type': 0.003343621399176877,\n",
       " 'treatment': 0.15332657885849366,\n",
       " 'tumour_type': 0.033977348434377164}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:44:10.155880Z",
     "start_time": "2024-09-11T08:44:10.145582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_kappa_score(split):\n",
    "    sorted_annotations = read_json(filePath)\n",
    "    split_set = pd.read_csv(\"Corpus-split - positive.csv\")\n",
    "    split_set = split_set[split_set['split'] == split]\n",
    "    df, entities = get_labels_for_all_annotator(split_annotations(sorted_annotations, split_set))\n",
    "    df = get_entities_labels_grouped(df, entities)\n",
    "    alpha_scores = kappa_score(df)\n",
    "    return alpha_scores\n",
    "\n",
    "def split_annotations(annotations, split):\n",
    "    split_anno = list()\n",
    "    for annotation in annotations:\n",
    "        sa = list()\n",
    "        for annotater in annotation:\n",
    "            if annotater['html'] in split['abstract'].to_list():\n",
    "                sa.append(annotater)\n",
    "        split_anno.append(sa)\n",
    "    return split_anno"
   ],
   "id": "c59307ea6a1d18e4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T08:26:18.635694Z",
     "start_time": "2024-08-27T08:26:18.373138Z"
    }
   },
   "cell_type": "code",
   "source": "get_kappa_score('dev')",
   "id": "9f4960464f48ec68",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'biomarker': 0.3177215189873418,\n",
       " 'cancer_grade': -0.11111111111111116,\n",
       " 'cancer_stage': 0.0,\n",
       " 'diagnosis': 0.24831081081081086,\n",
       " 'genetic_effect': 0.46153846153846156,\n",
       " 'host_strain': -0.25,\n",
       " 'model_type': 0.11980216156805279,\n",
       " 'molecular_char': 0.08994708994708989,\n",
       " 'response_to_treatment': 0.16746411483253587,\n",
       " 'sample_type': 0.375,\n",
       " 'treatment': 0.1457246085909274,\n",
       " 'tumour_type': 0.2222222222222222}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-27T08:27:40.057358Z",
     "start_time": "2024-08-27T08:27:39.716818Z"
    }
   },
   "cell_type": "code",
   "source": "get_kappa_score('test')",
   "id": "1935c81809398194",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biomarker\n",
      "clinical_trial\n",
      "diagnosis\n",
      "genetic_effect\n",
      "model_type\n",
      "molecular_char\n",
      "response_to_treatment\n",
      "sample_type\n",
      "treatment\n",
      "tumour_type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'biomarker': 0.08504854368932024,\n",
       " 'clinical_trial': 0.0,\n",
       " 'diagnosis': 0.19518963922294175,\n",
       " 'genetic_effect': 0.2100840336134454,\n",
       " 'model_type': 0.0472934472934472,\n",
       " 'molecular_char': 0.056774193548386975,\n",
       " 'response_to_treatment': -0.15999999999999992,\n",
       " 'sample_type': 0.034965034965035,\n",
       " 'treatment': 0.002941176470588114,\n",
       " 'tumour_type': 0.14814814814814814}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:51:58.094965Z",
     "start_time": "2024-09-11T08:51:56.461037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import combinations\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def create_binary_matrix(df, entity):\n",
    "    \"\"\"Create a binary matrix of annotations for a specific entity.\"\"\"\n",
    "    subset = df[df['entity'] == entity]\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    binary_matrix = mlb.fit_transform(subset['entity_label_text'])\n",
    "    return binary_matrix, mlb.classes_\n",
    "\n",
    "# Function to calculate F1 scores for each pair of annotators\n",
    "def calculate_pairwise_f1_scores(df, entity):\n",
    "    binary_matrix, labels = create_binary_matrix(df, entity)\n",
    "    annotator_pairs = list(combinations(range(binary_matrix.shape[0]), 2))  # Generate all pairs\n",
    "    f1_scores = {}\n",
    "    for (i, j) in annotator_pairs:\n",
    "        f1 = f1_score(binary_matrix[i], binary_matrix[j], average='macro')\n",
    "        f1_scores[(i+1, j+1)] = f1\n",
    "\n",
    "    return f1_scores\n",
    "\n",
    "sorted_annotations = read_json(filePath)\n",
    "split_set = pd.read_csv(\"Corpus-split - positive.csv\")\n",
    "split_set = split_set[split_set['split'] == \"train\"]\n",
    "df, entities = get_labels_for_all_annotator(split_annotations(sorted_annotations, split_set))\n",
    "df = get_entities_labels_grouped(df, entities)\n",
    "\n",
    "# Compute F1 scores for each entity\n",
    "all_f1_scores = {}\n",
    "entities = df['entity'].unique()\n",
    "\n",
    "for entity in entities:\n",
    "    f1_scores = calculate_pairwise_f1_scores(df, entity)\n",
    "    all_f1_scores[entity] = f1_scores\n",
    "\n",
    "# Display the F1 scores\n",
    "for entity, scores in all_f1_scores.items():\n",
    "    print(f\"F1 Scores for entity '{entity}':\")\n",
    "    for (annotator1, annotator2), f1 in scores.items():\n",
    "        print(f\"Annotator {annotator1} vs Annotator {annotator2}: F1 Score = {f1:.2f}\")\n"
   ],
   "id": "b65fe6c752e03cef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores for entity 'age_category':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.40\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.40\n",
      "Annotator 2 vs Annotator 3: F1 Score = 1.00\n",
      "F1 Scores for entity 'biomarker':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.57\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.61\n",
      "Annotator 2 vs Annotator 3: F1 Score = 0.67\n",
      "F1 Scores for entity 'cancer_grade':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.33\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.33\n",
      "Annotator 2 vs Annotator 3: F1 Score = 1.00\n",
      "F1 Scores for entity 'cancer_stage':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.83\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.25\n",
      "Annotator 2 vs Annotator 3: F1 Score = 0.49\n",
      "F1 Scores for entity 'clinical_trial':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.71\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.46\n",
      "Annotator 2 vs Annotator 3: F1 Score = 0.68\n",
      "F1 Scores for entity 'diagnosis':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.60\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.54\n",
      "Annotator 2 vs Annotator 3: F1 Score = 0.69\n",
      "F1 Scores for entity 'genetic_effect':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.64\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.52\n",
      "Annotator 2 vs Annotator 3: F1 Score = 0.44\n",
      "F1 Scores for entity 'host_strain':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.40\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.40\n",
      "Annotator 2 vs Annotator 3: F1 Score = 1.00\n",
      "F1 Scores for entity 'model_id':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 1.00\n",
      "Annotator 1 vs Annotator 3: F1 Score = 1.00\n",
      "Annotator 2 vs Annotator 3: F1 Score = 1.00\n",
      "F1 Scores for entity 'model_type':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.45\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.53\n",
      "Annotator 2 vs Annotator 3: F1 Score = 0.64\n",
      "F1 Scores for entity 'molecular_char':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.46\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.60\n",
      "Annotator 2 vs Annotator 3: F1 Score = 0.57\n",
      "F1 Scores for entity 'response_to_treatment':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.54\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.51\n",
      "Annotator 2 vs Annotator 3: F1 Score = 0.63\n",
      "F1 Scores for entity 'sample_type':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.84\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.33\n",
      "Annotator 2 vs Annotator 3: F1 Score = 0.36\n",
      "F1 Scores for entity 'treatment':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.53\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.48\n",
      "Annotator 2 vs Annotator 3: F1 Score = 0.67\n",
      "F1 Scores for entity 'tumour_type':\n",
      "Annotator 1 vs Annotator 2: F1 Score = 0.40\n",
      "Annotator 1 vs Annotator 3: F1 Score = 0.62\n",
      "Annotator 2 vs Annotator 3: F1 Score = 0.37\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:44:35.960343Z",
     "start_time": "2024-09-11T08:44:35.955436Z"
    }
   },
   "cell_type": "code",
   "source": "df.annotator.unique()",
   "id": "13fd4909831e09e3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:44:45.974245Z",
     "start_time": "2024-09-11T08:44:45.968094Z"
    }
   },
   "cell_type": "code",
   "source": "entities",
   "id": "d9495c3fb27d3ba8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['age_category', 'biomarker', 'cancer_grade', 'cancer_stage',\n",
       "       'clinical_trial', 'diagnosis', 'genetic_effect', 'host_strain',\n",
       "       'model_id', 'model_type', 'molecular_char',\n",
       "       'response_to_treatment', 'sample_type', 'treatment', 'tumour_type'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5c921eab400583d8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
